{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import unicodedata\n",
    "import MeCab  # https://pypi.org/project/mecab-python3/\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.vocabularies = set()\n",
    "        self.category_count = {}\n",
    "        self.category_words = {}\n",
    "        self.word_count = {}\n",
    "    \n",
    "    def _count_words(self, documents, min_df=0):\n",
    "        vec_count = CountVectorizer(min_df)\n",
    "        vec_count.fit([d.lower() for d in documents])\n",
    "        return dict(list(vec_count.vocabulary_.items()))\n",
    "    \n",
    "    def fit(self, category, documents):\n",
    "        if category in self.category_count:\n",
    "            raise Exception(\"%s はすでにあるぞい！\" % category)\n",
    "        \n",
    "        self.category_count[category] = len(documents)\n",
    "        self.category_words[category] = self._count_words(documents, 0)\n",
    "        \n",
    "        for word in self.category_words[category].keys():\n",
    "            self.vocabularies.add(word)\n",
    "            \n",
    "            if word not in self.word_count:\n",
    "                self.word_count[word] = 0\n",
    "            self.word_count[word] += self.category_words[category][word]\n",
    "            \n",
    "            \n",
    "    def classify(self, text):\n",
    "        word_counts = self._count_words([text])        \n",
    "        category_scores = {}\n",
    "        for category in self.category_count.keys():\n",
    "            score = np.log(self._category_p(category))\n",
    "            score += self._document_score(category, word_counts)\n",
    "            category_scores[category] = score\n",
    "        \n",
    "        n = sum(category_scores.values())\n",
    "        return {k: 1 - (v / n) for k, v in category_scores.items()}\n",
    "            \n",
    "    def _document_score(self, category, word_counts):\n",
    "        score = 0\n",
    "        for word in word_counts.keys():\n",
    "            p = self._word_p(category, word)\n",
    "            score += np.log(p) * word_counts[word]\n",
    "        return score\n",
    "            \n",
    "    def _word_p(self, category, word):\n",
    "        return float(\n",
    "            (self._get_word_count(category, word) + 1.0) \n",
    "            / (sum(self.category_words[category].values()) + float(len(self.vocabularies)))\n",
    "        )            \n",
    "            \n",
    "    def _get_word_count(self, category, word):\n",
    "        if word in self.category_words[category]:\n",
    "            return float(self.category_words[category][word])\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "    def _category_p(self, category):\n",
    "        return self.category_count[category] / sum(self.category_count.values())\n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "class NaiveBayes2:\n",
    "    def __init__(self):\n",
    "        self.vocabularies = set()\n",
    "        self.category_count = {}\n",
    "        self.category_words = {}\n",
    "        self.word_count = {}\n",
    "            \n",
    "    def fit(self, category, texts: list):\n",
    "        if category in self.category_count:\n",
    "            raise Exception(\"%s is already registered\" % category)\n",
    "        \n",
    "        self.category_count[category] = len(texts)\n",
    "        \n",
    "        concatenated_text = \" \".join(texts)\n",
    "        words = self.to_words(concatenated_text)\n",
    "        counts = self.count_words(words)\n",
    "        \n",
    "        vocabulary = set(words)\n",
    "        self.vocabularies |= vocabulary\n",
    "        self.category_words[category] = counts\n",
    "        \n",
    "        for w in vocabulary:\n",
    "            if w not in self.word_count:\n",
    "                self.word_count[w] = 0\n",
    "            self.word_count[w] += self.category_words[category][w]\n",
    "    \n",
    "    def count_words(self, words: list) -> dict:\n",
    "        counts = {k: 0 for k in set(words)}\n",
    "        for w in words:\n",
    "            counts[w] += 1\n",
    "        \n",
    "        return counts\n",
    "    \n",
    "    def to_words(self, text: str) -> list:\n",
    "        words = self._normalize_document(text)\n",
    "        words = self._separate_text(words)\n",
    "        return words\n",
    "            \n",
    "    def _normalize_document(self, text: str) -> str:\n",
    "        t = unicodedata.normalize(\"NFKC\", text)\n",
    "        t = t.lower()\n",
    "        t = re.sub('\\s+', ' ', t)\n",
    "        return t\n",
    "    \n",
    "    def _separate_text(self, text: str) -> list:\n",
    "        return wakati.parse(text).split()\n",
    "    \n",
    "    \n",
    "    def classify(self, text: str):\n",
    "        words = self.to_words(text)\n",
    "        counts = self.count_words(words)\n",
    "\n",
    "        category_scores = {}\n",
    "        for category in self.category_count.keys():\n",
    "            score = np.log(self._category_p(category))\n",
    "            score += self._document_score(category, counts)\n",
    "            category_scores[category] = score\n",
    "\n",
    "        n = sum(category_scores.values())\n",
    "        return {k: 1 - (v / n) for k, v in category_scores.items()}\n",
    "    \n",
    "    def _document_score(self, category, word_counts):\n",
    "        score = 0\n",
    "        for word in word_counts.keys():\n",
    "            p = self._word_p(category, word)\n",
    "            score += np.log(p) * word_counts[word]\n",
    "        return score\n",
    "            \n",
    "    def _word_p(self, category, word):\n",
    "        return float(\n",
    "            (self._get_word_count(category, word) + 1.0) \n",
    "            / (sum(self.category_words[category].values()) + float(len(self.vocabularies)))\n",
    "        )            \n",
    "            \n",
    "    def _get_word_count(self, category, word):\n",
    "        if word in self.category_words[category]:\n",
    "            return float(self.category_words[category][word])\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "    def _category_p(self, category):\n",
    "        return self.category_count[category] / sum(self.category_count.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count = 1000\n",
    "\n",
    "df = pd.read_table(\"./SMSSpamCollection.csv\", sep='\\t', header=None, names=['class', 'text'])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# display(df)\n",
    "\n",
    "test_df = df[0:test_count]\n",
    "df = df[test_count:].reset_index(drop=True)\n",
    "\n",
    "obj = NaiveBayes()\n",
    "obj.fit(\"spam\", df[\"text\"][df[\"class\"] == \"spam\"])\n",
    "obj.fit(\"ham\", df[\"text\"][df[\"class\"] == \"ham\"])\n",
    "\n",
    "a = NaiveBayes2()\n",
    "a.fit(\"spam\", list(df[\"text\"][df[\"class\"] == \"spam\"]))\n",
    "a.fit(\"ham\", list(df[\"text\"][df[\"class\"] == \"ham\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj.vocabularies = a.vocabularies\n",
    "# obj.category_count = a.category_count\n",
    "# obj.category_words = a.category_words\n",
    "# obj.word_count = a.word_count\n",
    "obj = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0.9695782661437988\n",
      "\n",
      "Train:  4572\n",
      "SPAM count 159\n",
      "HAM count 841\n",
      "\n",
      "N: 1000\n",
      "OK: 154\n",
      "NG: 10\n",
      "0.9390243329863212\n",
      "0.154\n"
     ]
    }
   ],
   "source": [
    "ok_count = 0\n",
    "ng_count = 0\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "N = 0\n",
    "for doc in test_df.iloc:\n",
    "    try:\n",
    "        p = obj.classify(doc[\"text\"])\n",
    "        spam_rate = p[\"spam\"]\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    N += 1\n",
    "#     print(spam_rate)\n",
    "    if spam_rate < 0.5:\n",
    "        continue\n",
    "        \n",
    "    if doc[\"class\"] == \"spam\":\n",
    "        ok_count += 1\n",
    "    else:\n",
    "        ng_count += 1\n",
    "        \n",
    "\n",
    "print(\"t=\", time.time() - st)\n",
    "print()\n",
    "\n",
    "print(\"Train: \", len(df))\n",
    "print(\"SPAM count\",len(test_df[\"text\"][test_df[\"class\"] == \"spam\"]))\n",
    "print(\"HAM count\",len(test_df[\"text\"][test_df[\"class\"] == \"ham\"]))\n",
    "print()\n",
    "\n",
    "print(\"N:\", N)\n",
    "print(\"OK:\", ok_count)\n",
    "print(\"NG:\", ng_count)\n",
    "print(ok_count / (ok_count + ng_count + 0.00001))\n",
    "print(ok_count / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spam': -151.5081900948173, 'ham': -168.82044184281256}\n",
      "This text is SPAM !!!\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "hello.\n",
    "my name is issei katayama\n",
    "\"\"\"\n",
    "\n",
    "scores = obj.classify(text.replace(\"\\n\", \"\"))\n",
    "\n",
    "print(scores)\n",
    "if scores[\"spam\"] > scores[\"ham\"]:\n",
    "    print(\"This text is SPAM !!!\")\n",
    "else:\n",
    "    print(\"The text is HAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
